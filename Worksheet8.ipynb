{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Audio Machine Learning - Workshop Week 8\n",
    "Pre-Trained Audio Feature Extraction Models\n",
    "\n",
    "This worksheet demonstrates feature extraction using a pre-trained VGGish audio classification model.\n",
    "\n",
    "This is intended as a simple demonstration. You should apply feature extraction to your own datasets, and also explore using other pre-trained models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68a237a1a09f0cbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0 - Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b654892b03dcca"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: resampy in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (0.4.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from resampy) (1.26.4)\r\n",
      "Requirement already satisfied: numba>=0.53 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from resampy) (0.61.0)\r\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from numba>=0.53->resampy) (0.44.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: soundfile in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (0.13.1)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from soundfile) (1.17.1)\r\n",
      "Requirement already satisfied: numpy in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from soundfile) (1.26.4)\r\n",
      "Requirement already satisfied: pycparser in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.22)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting datasets\r\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: filelock in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from datasets) (3.17.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from datasets) (1.26.4)\r\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\r\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\r\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\r\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: pandas in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from datasets) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from datasets) (4.67.1)\r\n",
      "Collecting xxhash (from datasets)\r\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\r\n",
      "Collecting multiprocess<0.70.17 (from datasets)\r\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\r\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting aiohttp (from datasets)\r\n",
      "  Downloading aiohttp-3.11.13-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from datasets) (0.29.3)\r\n",
      "Requirement already satisfied: packaging in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from datasets) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from datasets) (6.0.2)\r\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\r\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\r\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from aiohttp->datasets) (25.1.0)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\r\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\r\n",
      "  Downloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.0 kB)\r\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\r\n",
      "  Downloading propcache-0.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (10 kB)\r\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\r\n",
      "  Downloading yarl-1.18.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (69 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/awrigh2/miniforge3/envs/noteableEnv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\r\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\r\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\r\n",
      "Downloading aiohttp-3.11.13-cp312-cp312-macosx_11_0_arm64.whl (456 kB)\r\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\r\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-macosx_12_0_arm64.whl (30.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.7/30.7 MB\u001B[0m \u001B[31m38.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\r\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\r\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\r\n",
      "Downloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\r\n",
      "Downloading propcache-0.3.0-cp312-cp312-macosx_11_0_arm64.whl (45 kB)\r\n",
      "Downloading yarl-1.18.3-cp312-cp312-macosx_11_0_arm64.whl (92 kB)\r\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2025.2.0\r\n",
      "    Uninstalling fsspec-2025.2.0:\r\n",
      "      Successfully uninstalled fsspec-2025.2.0\r\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.13 aiosignal-1.3.2 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.3.0 pyarrow-19.0.1 xxhash-3.5.0 yarl-1.18.3\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install resampy\n",
    "%pip install soundfile\n",
    "%pip install datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-13T16:19:53.387832Z",
     "start_time": "2025-03-13T16:19:40.880348Z"
    }
   },
   "id": "cdcb0ce3f7665a09",
   "execution_count": 146
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import resampy\n",
    "import soundfile\n",
    "from transformers import ClapConfig, ClapModel\n",
    "import librosa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-13T16:20:11.762219Z",
     "start_time": "2025-03-13T16:20:11.758460Z"
    }
   },
   "id": "f781189304dd404b",
   "execution_count": 149
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 - Load VGGISH Model\n",
    "\n",
    "VGGISH is a convolutional neural network model for large scale audio classification. You can read more about it here:\n",
    "\n",
    "https://arxiv.org/abs/1609.09430\n",
    "\n",
    "The model is trained on the AudioSet dataset, a large scale audio dataset taken from YouTube videos:\n",
    "\n",
    "https://research.google.com/audioset/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d37c464209620"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pre-trained models are available from a variety of sources. torch hub is one way of loading pre-trained models:\n",
    "\n",
    "https://pytorch.org/docs/stable/hub.html#loading-models-from-hub\n",
    "\n",
    "Below is an example that loads a torch VGGish model, from the GitHub repo:\n",
    "\n",
    "https://github.com/harritaylor/torchvggish"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "396fe2d07b40685f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/awrigh2/.cache/torch/hub/harritaylor_torchvggish_master\n"
     ]
    },
    {
     "data": {
      "text/plain": "VGGish(\n  (features): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (12): ReLU(inplace=True)\n    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (14): ReLU(inplace=True)\n    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (embeddings): Sequential(\n    (0): Linear(in_features=12288, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Linear(in_features=4096, out_features=4096, bias=True)\n    (3): ReLU(inplace=True)\n    (4): Linear(in_features=4096, out_features=128, bias=True)\n    (5): ReLU(inplace=True)\n  )\n  (pproc): Postprocessor()\n)"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('harritaylor/torchvggish', 'vggish') # The arguments are the github repo that hosts the model, and the name of the model\n",
    "#This just removes the output ReLU layer from the model\n",
    "model.postprocess = False \n",
    "model.embeddings = torch.nn.Sequential(*list(model.embeddings.children())[:-1])\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-13T15:56:49.248501Z",
     "start_time": "2025-03-13T15:56:48.495410Z"
    }
   },
   "id": "87310f7a726b01b2",
   "execution_count": 125
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 - Explore Model\n",
    "\n",
    "Now you can look at the model code:\n",
    "\n",
    "https://github.com/harritaylor/torchvggish/blob/master/torchvggish/vggish.py\n",
    "\n",
    "Under the VGGish class you can see what happens when the model 'forward' method is called. You can also use debug mode to step through the model when it is processing inputs. In this case, the VGGish model can accept inputs as 1-d numpy arrays, and it will pre-process them automatically into log mel spectrograms in the 'waveform_to_examples' function.\n",
    "\n",
    "I HIGHLY recommend that you look through these functions and classes on the GitHub page."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c51c5df54202507"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dummy_audio = np.random.randn(16000) # Make 1-second of audio at 16kHz sample rate\n",
    "\n",
    "dummy_feats = model.forward(x=dummy_audio, fs=16000) # Call the model on the dummy audio"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-13T16:07:39.829962Z",
     "start_time": "2025-03-13T16:07:39.739399Z"
    }
   },
   "id": "37441eabdcb314b4",
   "execution_count": 132
  },
  {
   "cell_type": "markdown",
   "source": [
    "Look at the features the model extracted - what dimensions do they have? These are the features extracted from the output layer of the VGGish model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "601c473c54974eeb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128])"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-13T16:07:44.933133Z",
     "start_time": "2025-03-13T16:07:44.925504Z"
    }
   },
   "id": "b7f5be2f4937ef8e",
   "execution_count": 133
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 - Feature Extraction\n",
    "\n",
    "Here I have used the VGGish model to extract features from the digit classification dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45585fc423d86092"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_audio = []\n",
    "train_labels = []\n",
    "train_embeddings = []\n",
    "test_audio = []\n",
    "test_labels = []\n",
    "test_embeddings = []\n",
    "for n in glob('DigitData/*.wav'):\n",
    "    file_name = n.split('/')[-1]\n",
    "    label = int(file_name[0])\n",
    "    id = int(file_name.split('_')[-1].split('.')[0])\n",
    "    train = True if id < 40 else False # Train/Test Split 80/20\n",
    "    \n",
    "    # Load Audio and append to train/test list\n",
    "    audio, fs = librosa.load(n)\n",
    "    if train:\n",
    "        train_audio.append(audio)\n",
    "        train_labels.append(label)\n",
    "    else:\n",
    "        test_audio.append(audio)\n",
    "        test_labels.append(label)\n",
    "    \n",
    "    # Make audio the same length - 1 second long\n",
    "    if audio.shape[0] < fs:\n",
    "        audio = np.concatenate((audio, np.zeros((fs - audio.shape[0]))))\n",
    "    elif audio.shape[0] > fs:\n",
    "        audio = audio[0:fs]\n",
    "        \n",
    "    with torch.inference_mode(): # Inference mode saves computation as it disables gradient tracking\n",
    "        feats = model.forward(audio, fs)\n",
    "\n",
    "    if train:\n",
    "        train_embeddings.append(feats)\n",
    "    else:\n",
    "        test_embeddings.append(feats)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-13T15:57:05.834011Z",
     "start_time": "2025-03-13T15:56:52.663843Z"
    }
   },
   "id": "29fdc39f9b611106",
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_emb_np = torch.stack(train_embeddings, dim=0)\n",
    "train_emb_np = train_emb_np.squeeze().numpy()\n",
    "train_labels_np = np.stack(train_labels)\n",
    "\n",
    "test_emb_np = torch.stack(test_embeddings, dim=0)\n",
    "test_emb_np = test_emb_np.squeeze().numpy()\n",
    "test_labels_np = np.stack(test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-13T15:57:05.842933Z",
     "start_time": "2025-03-13T15:57:05.836431Z"
    }
   },
   "id": "89df22c8afeadc38",
   "execution_count": 127
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 - Model Fitting\n",
    "\n",
    "The below code fits a K nearest neighbours on the above data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95e11123ad376c35"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "KNeighborsClassifier()",
      "text/html": "<style>#sk-container-id-10 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-10 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-10 pre {\n  padding: 0;\n}\n\n#sk-container-id-10 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-10 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-10 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-10 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-10 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-10 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-10 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-10 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-10 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-10 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-10 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-10 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-10 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-10 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-10 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-10 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-10 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n#sk-container-id-10 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-10 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-10 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-10 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-10 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-10 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-10 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-10 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-10 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-10 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KNeighborsClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier()</pre></div> </div></div></div></div>"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = KNeighborsClassifier(n_neighbors=5)\n",
    "nn.fit(train_emb_np, train_labels_np)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-13T16:11:22.445747Z",
     "start_time": "2025-03-13T16:11:22.425202Z"
    }
   },
   "id": "594655061f775f00",
   "execution_count": 141
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "preds = nn.predict(test_emb_np)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-13T16:11:23.462084Z",
     "start_time": "2025-03-13T16:11:23.457668Z"
    }
   },
   "id": "9a2d6e2d355fc3b5",
   "execution_count": 142
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 79.0 %!\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy is {100*sum(test_labels_np == preds)/test_labels_np.shape[0]} %!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-13T16:11:53.544749Z",
     "start_time": "2025-03-13T16:11:53.526203Z"
    }
   },
   "id": "4e1f28ec1510109a",
   "execution_count": 145
  },
  {
   "cell_type": "markdown",
   "source": [
    "This should get accuracy of around 80%. Whilst not perfect, it does demonstrate that the features extracted by VGGish are useful for downstream audio classification tasks, especially considered that VGGish wasn't trained on the task of speech recognition at all!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3baf66410891f928"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 - Further Work\n",
    "\n",
    "This worksheet was mostly a demonstration of using an audio embedding model for classification. You should try and modify your own feature extraction code so you have the option of using VGGish embeddings as a feature. \n",
    "\n",
    "There are also many other pre-trained models available, from torch hub, but also from other sources like 'Hugging Face':\n",
    "\n",
    "https://huggingface.co/models\n",
    "\n",
    "There are many models out there:\n",
    "\n",
    "CLAP is multi-modal model that creates embeddings from text or audio. You can use this for supervised audio classification, but you can also use it to for text-to-audio retrieval.\n",
    "https://github.com/LAION-AI/CLAP\n",
    "\n",
    "An example of using a Hugging Face CLAP model is found on this page:\n",
    "\n",
    "https://huggingface.co/laion/larger_clap_music\n",
    "\n",
    "I encourage you to look through some of the audio models available on Hugging Face:\n",
    "\n",
    "https://huggingface.co/docs/transformers/index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59d3d9ad2631493a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "34c6223b531419ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
