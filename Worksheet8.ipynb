{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Audio Machine Learning - Workshop Week 8\n",
    "Pre-Trained Audio Feature Extraction Models\n",
    "\n",
    "This worksheet demonstrates feature extraction using a pre-trained VGGish audio classification model.\n",
    "\n",
    "This is intended as a simple demonstration. You should apply feature extraction to your own datasets, and also explore using other pre-trained models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68a237a1a09f0cbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0 - Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b654892b03dcca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%pip install resampy\n",
    "%pip install soundfile\n",
    "%pip install datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdcb0ce3f7665a09",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import resampy\n",
    "import soundfile\n",
    "from transformers import ClapConfig, ClapModel\n",
    "import librosa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f781189304dd404b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 - Load VGGISH Model\n",
    "\n",
    "VGGISH is a convolutional neural network model for large scale audio classification. You can read more about it here:\n",
    "\n",
    "https://arxiv.org/abs/1609.09430\n",
    "\n",
    "The model is trained on the AudioSet dataset, a large scale audio dataset taken from YouTube videos:\n",
    "\n",
    "https://research.google.com/audioset/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d37c464209620"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pre-trained models are available from a variety of sources. torch hub is one way of loading pre-trained models:\n",
    "\n",
    "https://pytorch.org/docs/stable/hub.html#loading-models-from-hub\n",
    "\n",
    "Below is an example that loads a torch VGGish model, from the GitHub repo:\n",
    "\n",
    "https://github.com/harritaylor/torchvggish"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "396fe2d07b40685f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = torch.hub.load('harritaylor/torchvggish', 'vggish') # The arguments are the github repo that hosts the model, and the name of the model\n",
    "#This just removes the output ReLU layer from the model\n",
    "model.postprocess = False \n",
    "model.embeddings = torch.nn.Sequential(*list(model.embeddings.children())[:-1])\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87310f7a726b01b2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 - Explore Model\n",
    "\n",
    "Now you can look at the model code:\n",
    "\n",
    "https://github.com/harritaylor/torchvggish/blob/master/torchvggish/vggish.py\n",
    "\n",
    "Under the VGGish class you can see what happens when the model 'forward' method is called. You can also use debug mode to step through the model when it is processing inputs. In this case, the VGGish model can accept inputs as 1-d numpy arrays, and it will pre-process them automatically into log mel spectrograms in the 'waveform_to_examples' function.\n",
    "\n",
    "I HIGHLY recommend that you look through these functions and classes on the GitHub page."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c51c5df54202507"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dummy_audio = np.random.randn(16000) # Make 1-second of audio at 16kHz sample rate\n",
    "\n",
    "dummy_feats = model.forward(x=dummy_audio, fs=16000) # Call the model on the dummy audio"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37441eabdcb314b4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Look at the features the model extracted - what dimensions do they have? These are the features extracted from the output layer of the VGGish model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "601c473c54974eeb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f5be2f4937ef8e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 - Feature Extraction\n",
    "\n",
    "Here I have used the VGGish model to extract features from the digit classification dataset.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45585fc423d86092"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_audio = []\n",
    "train_labels = []\n",
    "train_embeddings = []\n",
    "test_audio = []\n",
    "test_labels = []\n",
    "test_embeddings = []\n",
    "for n in glob('DigitData/*.wav'):\n",
    "    file_name = n.split('/')[-1]\n",
    "    label = int(file_name[0])\n",
    "    id = int(file_name.split('_')[-1].split('.')[0])\n",
    "    train = True if id < 40 else False # Train/Test Split 80/20\n",
    "    \n",
    "    # Load Audio and append to train/test list\n",
    "    audio, fs = librosa.load(n)\n",
    "    if train:\n",
    "        train_audio.append(audio)\n",
    "        train_labels.append(label)\n",
    "    else:\n",
    "        test_audio.append(audio)\n",
    "        test_labels.append(label)\n",
    "    \n",
    "    # Make audio the same length - 1 second long\n",
    "    if audio.shape[0] < fs:\n",
    "        audio = np.concatenate((audio, np.zeros((fs - audio.shape[0]))))\n",
    "    elif audio.shape[0] > fs:\n",
    "        audio = audio[0:fs]\n",
    "        \n",
    "    with torch.inference_mode(): # Inference mode saves computation as it disables gradient tracking\n",
    "        feats = model.forward(audio, fs)\n",
    "\n",
    "    if train:\n",
    "        train_embeddings.append(feats)\n",
    "    else:\n",
    "        test_embeddings.append(feats)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29fdc39f9b611106",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_emb_np = torch.stack(train_embeddings, dim=0)\n",
    "train_emb_np = train_emb_np.squeeze().numpy()\n",
    "train_labels_np = np.stack(train_labels)\n",
    "\n",
    "test_emb_np = torch.stack(test_embeddings, dim=0)\n",
    "test_emb_np = test_emb_np.squeeze().numpy()\n",
    "test_labels_np = np.stack(test_labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89df22c8afeadc38",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 - Model Fitting\n",
    "\n",
    "The below code fits a K nearest neighbours on the above data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95e11123ad376c35"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "nn = KNeighborsClassifier(n_neighbors=5)\n",
    "nn.fit(train_emb_np, train_labels_np)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "594655061f775f00",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "preds = nn.predict(test_emb_np)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a2d6e2d355fc3b5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'Accuracy is {100*sum(test_labels_np == preds)/test_labels_np.shape[0]} %!')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e1f28ec1510109a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This should get accuracy of around 80%. Whilst not perfect, it does demonstrate that the features extracted by VGGish are useful for downstream audio classification tasks, especially considered that VGGish wasn't trained on the task of speech recognition at all!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3baf66410891f928"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 - Further Work\n",
    "\n",
    "This worksheet was mostly a demonstration of using an audio embedding model for classification. You should try and modify your own feature extraction code so you have the option of using VGGish embeddings as a feature. \n",
    "\n",
    "There are also many other pre-trained models available, from torch hub, but also from other sources like 'Hugging Face':\n",
    "\n",
    "https://huggingface.co/models\n",
    "\n",
    "There are many models out there:\n",
    "\n",
    "CLAP is multi-modal model that creates embeddings from text or audio. You can use this for supervised audio classification, but you can also use it to for text-to-audio retrieval.\n",
    "https://github.com/LAION-AI/CLAP\n",
    "\n",
    "An example of using a Hugging Face CLAP model is found on this page:\n",
    "\n",
    "https://huggingface.co/laion/larger_clap_music\n",
    "\n",
    "I encourage you to look through some of the audio models available on Hugging Face:\n",
    "\n",
    "https://huggingface.co/docs/transformers/index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59d3d9ad2631493a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "34c6223b531419ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
